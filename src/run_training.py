import os
import sys

import happybase
from pyspark.sql import SparkSession
from pyspark.sql.types import (FloatType, IntegerType, LongType, StructField,
                               StructType)

# --- SETUP PATH ---
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.append(project_root)

from configs import config
from src.models.als_recommender import ALSRecommender


def get_hbase_conn():
    """HÃ m táº¡o káº¿t ná»‘i HBase (Ä‘Æ°á»£c gá»i bÃªn trong tá»«ng Worker cá»§a Spark)"""
    return happybase.Connection(config.HBASE_HOST, timeout=10000)

def save_partition_to_hbase(iterator):
    """
    HÃ m nÃ y cháº¡y trÃªn tá»«ng Partition cá»§a RDD Ä‘á»ƒ ghi dá»¯ liá»‡u xuá»‘ng HBase.
    Má»—i partition má»Ÿ 1 káº¿t ná»‘i Ä‘á»ƒ tá»‘i Æ°u hiá»‡u nÄƒng.
    """
    conn = None
    try:
        conn = get_hbase_conn()
        table = conn.table(config.HBASE_TABLE_RECS)
        batch = table.batch(batch_size=1000)
        
        # TÃªn cá»™t chá»©a danh sÃ¡ch gá»£i Ã½
        col_family = b'info:movieIds'

        for row in iterator:
            # row dáº¡ng: (userId, recommendations=[Row(movieId, rating), ...])
            user_id = str(row.userId)
            
            clean_recs = []
            
            # [LOGIC QUAN TRá»ŒNG] Xá»­ lÃ½ tá»«ng gá»£i Ã½
            for rec in row.recommendations:
                # 1. Láº¥y Ä‘iá»ƒm dá»± Ä‘oÃ¡n thÃ´
                raw_rating = float(rec.rating)
                
                # 2. Cáº®T NGá»ŒN (CLIPPING): Ã‰p vá» khoáº£ng [0.0, 5.0]
                clean_rating = max(0.0, min(5.0, raw_rating))
                
                # 3. Format chuá»—i "MovieID:Rating"
                clean_recs.append(f"{rec.movieId}:{clean_rating:.2f}")

            # Ná»‘i láº¡i thÃ nh chuá»—i ngÄƒn cÃ¡ch bá»Ÿi dáº¥u pháº©y
            rec_string = ",".join(clean_recs)
            
            # ÄÆ°a vÃ o Batch
            batch.put(user_id.encode(), {col_family: rec_string.encode()})

        # Gá»­i dá»¯ liá»‡u Ä‘i
        batch.send()
        
    except Exception as e:
        print(f"âŒ Error writing to HBase: {e}")
    finally:
        if conn:
            conn.close()

def main():
    print("ğŸš€ Starting Spark ALS Training Pipeline...")
    
    # 1. Khá»Ÿi táº¡o Spark Session
    spark = SparkSession.builder \
        .appName("MovieLens_ALS_Training") \
        .master("local[*]") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")

    # 2. Äá»c dá»¯ liá»‡u tá»« FILE CSV
    raw_csv_path = os.path.join(config.DATA_DIR_LOCAL, config.RATINGS_FILE)
    
    # [FIX QUAN TRá»ŒNG] ThÃªm prefix 'file://' Ä‘á»ƒ Spark biáº¿t Ä‘Ã¢y lÃ  local file
    csv_path = f"file://{raw_csv_path}"
    
    if not os.path.exists(raw_csv_path):
        print(f"âŒ File not found: {raw_csv_path}")
        return

    print(f"ğŸ“‚ Reading raw data from: {csv_path}")
    
    # Äá»‹nh nghÄ©a Schema rÃµ rÃ ng Ä‘á»ƒ tá»‘i Æ°u hiá»‡u nÄƒng
    schema = StructType([
        StructField("userId", IntegerType(), True),
        StructField("movieId", IntegerType(), True),
        StructField("rating", FloatType(), True),
        StructField("timestamp", LongType(), True)
    ])
    
    df_ratings = spark.read.csv(csv_path, header=True, schema=schema)
    
    # Cache dá»¯ liá»‡u Ä‘á»ƒ train nhanh hÆ¡n
    df_ratings.cache()
    print(f"ğŸ“Š Training data loaded. Total rows: {df_ratings.count()}")

    # 3. Train Model
    print("ğŸ§  Training ALS Model...")
    recommender = ALSRecommender(spark)
    recommender.train(df_ratings)
    
    # 4. Táº¡o gá»£i Ã½ (Top 10 phim cho Má»ŒI User)
    print("ğŸ”® Generating recommendations for all users...")
    user_recs = recommender.get_recommendations(k=10)
    
    # 5. LÆ°u vÃ o HBase (Sá»­ dá»¥ng hÃ m foreachPartition)
    print(f"ğŸ’¾ Saving recommendations to HBase table: {config.HBASE_TABLE_RECS}...")
    
    # Kiá»ƒm tra báº£ng cÃ³ tá»“n táº¡i khÃ´ng trÆ°á»›c khi ghi (Optional)
    try:
        tmp_conn = happybase.Connection(config.HBASE_HOST)
        # Fix lá»—i decode náº¿u tÃªn báº£ng dáº¡ng bytes
        tables = [t.decode('utf-8') for t in tmp_conn.tables()]
        if config.HBASE_TABLE_RECS not in tables:
            print(f"ğŸ›  Creating table {config.HBASE_TABLE_RECS}...")
            tmp_conn.create_table(config.HBASE_TABLE_RECS, {'info': dict()})
        tmp_conn.close()
    except Exception as e:
        print(f"âš ï¸ Warning checking table: {e}")

    # Ghi dá»¯ liá»‡u phÃ¢n tÃ¡n
    user_recs.foreachPartition(save_partition_to_hbase)
    
    print("âœ… Training Pipeline Completed Successfully!")
    spark.stop()

if __name__ == "__main__":
    main()