import os
import subprocess
import sys

import happybase

# Setup ƒë∆∞·ªùng d·∫´n
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from configs import config


def get_hbase_connection():
    try:
        connection = happybase.Connection(config.HBASE_HOST, timeout=30000, autoconnect=True)
        return connection
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi HBase: {e}")
        return None

def load_from_hdfs(connection, hdfs_path, column_family, column_name, description):
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu n·∫°p '{description}' t·ª´: {hdfs_path}")
    
    table = connection.table(config.HBASE_TABLE_MOVIES)
    
    # L·ªánh ƒë·ªçc file t·ª´ HDFS
    hdfs_cmd = f"hdfs dfs -cat {hdfs_path}/part-*"
    
    try:
        process = subprocess.Popen(hdfs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        batch_size = 500
        batch = table.batch(batch_size=batch_size)
        count = 0
        
        for line in process.stdout:
            try:
                line_str = line.decode('utf-8').strip()
                if not line_str: continue

                # T√°ch b·∫±ng d·∫•u TAB (\t) do MapReduce xu·∫•t ra
                parts = line_str.split('\t')
                
                # Fallback d·∫•u ph·∫©y
                if len(parts) < 2:
                    parts = line_str.split(',')
                
                if len(parts) >= 2:
                    movie_id = parts[0].strip()
                    value = parts[1].strip()
                    
                    # Ghi v√†o HBase
                    # column_family: b'stats'
                    # column_name: b'avg_rating' ho·∫∑c b'rating_count'
                    col_key = f"{column_family}:{column_name}".encode()
                    
                    batch.put(movie_id.encode(), {
                        col_key: value.encode()
                    })
                    count += 1
                    
                    if count % 2000 == 0:
                        print(f"   -> ƒê√£ n·∫°p {count} d√≤ng...", end='\r')
                        
            except Exception as e:
                continue

        batch.send()
        print(f"‚úÖ HO√ÄN T·∫§T '{description}'! T·ªïng c·ªông: {count} d√≤ng.")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc HDFS: {e}")

def main():
    conn = get_hbase_connection()
    if not conn: return

    try:
        # 1. N·∫†P AVG RATING (ƒêi·ªÉm c·ªông ƒë·ªìng)
        # MapReduce Job: Average Ratings
        load_from_hdfs(
            conn, 
            config.HDFS_OUTPUT_AVG, 
            "stats", 
            "avg_rating", 
            "ƒêi·ªÉm C·ªông ƒê·ªìng"
        )

        # 2. N·∫†P RATING COUNT (S·ªë l∆∞·ª£t ƒë√°nh gi√°)
        # MapReduce Job: Count Ratings
        load_from_hdfs(
            conn, 
            config.HDFS_OUTPUT_RATINGS, 
            "stats", 
            "rating_count", 
            "S·ªë L∆∞·ª£t ƒê√°nh Gi√°"
        )

    finally:
        conn.close()

if __name__ == "__main__":
    main()