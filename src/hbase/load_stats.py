import os
import subprocess
import sys

import happybase
import pandas as pd  # <--- C·∫¶N IMPORT PANDAS

# Setup ƒë∆∞·ªùng d·∫´n
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(current_dir))
sys.path.append(project_root)

from configs import config


def get_hbase_connection():
    try:
        # TƒÉng timeout n·∫øu n·∫°p d·ªØ li·ªáu l·ªõn
        connection = happybase.Connection(config.HBASE_HOST, port=9090, timeout=60000, autoconnect=True)
        return connection
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi HBase: {e}")
        return None

# --- JOB 1 & 2: N·∫°p output c·ªßa MapReduce t·ª´ HDFS v√†o b·∫£ng movies ---
def load_from_hdfs(connection, hdfs_path, column_family, column_name, description):
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu n·∫°p '{description}' t·ª´ HDFS: {hdfs_path}")
    
    # Ki·ªÉm tra b·∫£ng t·ªìn t·∫°i
    if config.HBASE_TABLE_MOVIES.encode() not in connection.tables():
        print(f"‚ùå L·ªói: B·∫£ng '{config.HBASE_TABLE_MOVIES}' kh√¥ng t·ªìn t·∫°i.")
        return

    table = connection.table(config.HBASE_TABLE_MOVIES)
    
    # L·ªánh ƒë·ªçc file t·ª´ HDFS
    hdfs_cmd = f"hdfs dfs -cat {hdfs_path}/part-*"
    
    try:
        process = subprocess.Popen(hdfs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        
        batch_size = 1000 # TƒÉng batch size l√™n m·ªôt ch√∫t ƒë·ªÉ nhanh h∆°n
        batch = table.batch(batch_size=batch_size)
        count = 0
        
        print("   -> ƒêang ƒë·ªçc stream t·ª´ HDFS v√† ghi v√†o HBase...")
        for line in process.stdout:
            try:
                line_str = line.decode('utf-8').strip()
                if not line_str: continue

                # T√°ch b·∫±ng d·∫•u TAB (\t) do MapReduce xu·∫•t ra
                parts = line_str.split('\t')
                
                # Fallback d·∫•u ph·∫©y (ƒë·ªÅ ph√≤ng)
                if len(parts) < 2:
                    parts = line_str.split(',')
                
                if len(parts) >= 2:
                    movie_id = parts[0].strip()
                    value = parts[1].strip()
                    
                    # Ghi v√†o HBase
                    col_key = f"{column_family}:{column_name}".encode()
                    
                    batch.put(movie_id.encode(), {
                        col_key: value.encode()
                    })
                    count += 1
                    
                    if count % 5000 == 0:
                        print(f"   -> ƒê√£ x·ª≠ l√Ω {count} d√≤ng...", end='\r')
                        
            except Exception as e:
                continue

        batch.send()
        print(f"\n‚úÖ HO√ÄN T·∫§T '{description}'! T·ªïng c·ªông: {count} d√≤ng ƒë∆∞·ª£c c·∫≠p nh·∫≠t.")
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc HDFS ho·∫∑c ghi batch: {e}")

# --- JOB 3: T√≠nh to√°n ph√¢n b·ªë rating t·ª´ CSV v√† n·∫°p v√†o b·∫£ng rating_stats ---
def load_rating_distribution_from_csv(connection):
    description = "Ph√¢n B·ªë Rating To√†n C·ª•c"
    csv_path = os.path.join(config.DATA_DIR_LOCAL, config.RATINGS_FILE)
    # Gi·∫£ s·ª≠ b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a HBASE_TABLE_RATING_STATS trong config
    table_name = config.HBASE_TABLE_RATING_STATS
    row_key = b'GLOBAL_DIST'
    cf = b'info'

    print(f"\nüöÄ B·∫Øt ƒë·∫ßu t√≠nh to√°n v√† n·∫°p '{description}' t·ª´ file CSV local: {csv_path}")

    # 1. Ki·ªÉm tra file CSV
    if not os.path.exists(csv_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file CSV t·∫°i '{csv_path}'. Ki·ªÉm tra l·∫°i config.")
        return

    # 2. Ki·ªÉm tra b·∫£ng HBase
    if table_name.encode() not in connection.tables():
         print(f"‚ùå L·ªói: B·∫£ng '{table_name}' ch∆∞a t·ªìn t·∫°i. Vui l√≤ng t·∫°o b·∫£ng trong HBase Shell tr∆∞·ªõc: create '{table_name}', '{cf.decode()}'")
         return

    try:
        # 3. D√πng Pandas ƒë·ªÉ t√≠nh to√°n nhanh
        print("   -> ƒêang ƒë·ªçc CSV v√† t√≠nh to√°n b·∫±ng Pandas (c√≥ th·ªÉ m·∫•t v√†i gi√¢y)...")
        # Ch·ªâ ƒë·ªçc c·ªôt 'rating' ƒë·ªÉ ti·∫øt ki·ªám RAM
        df = pd.read_csv(csv_path, usecols=['rating'])
        # ƒê·∫øm s·ªë l∆∞·ª£ng (value_counts) v√† s·∫Øp x·∫øp theo index (m·ª©c rating)
        rating_counts = df['rating'].value_counts().sort_index()

        print(f"   -> ƒê√£ t√≠nh xong. T√¨m th·∫•y {len(rating_counts)} m·ª©c rating kh√°c nhau.")

        # 4. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ Put v√†o HBase
        table = connection.table(table_name)
        hbase_data = {}
        for rating_val, count in rating_counts.items():
            rating_str = str(rating_val)
            col_key = f"{cf.decode()}:{rating_str}".encode('utf-8')
            col_value = str(count).encode('utf-8')
            hbase_data[col_key] = col_value

        # 5. Th·ª±c hi·ªán 1 l·ªánh Put duy nh·∫•t
        print(f"   -> ƒêang ghi d·ªØ li·ªáu v√†o b·∫£ng '{table_name}' v·ªõi RowKey '{row_key.decode()}'...")
        table.put(row_key, hbase_data)
        print(f"‚úÖ HO√ÄN T·∫§T '{description}'!")

    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω CSV ho·∫∑c ghi HBase: {e}")

# --- JOB 4 (M·ªöI): T√≠nh to√°n t·ªïng quan h·ªá th·ªëng v√† n·∫°p v√†o b·∫£ng system_stats ---
def load_system_overview_from_csv(connection):
    description = "T·ªïng Quan H·ªá Th·ªëng (Users, Movies, Ratings)"
    movies_csv_path = os.path.join(config.DATA_DIR_LOCAL, config.MOVIES_FILE)
    ratings_csv_path = os.path.join(config.DATA_DIR_LOCAL, config.RATINGS_FILE)

    # C·∫¶N ƒê·∫¢M B·∫¢O BI·∫æN N√ÄY C√ì TRONG FILE CONFIG C·ª¶A B·∫†N
    table_name = config.HBASE_TABLE_SYSTEM_STATS
    row_key = b'OVERVIEW'
    cf = b'info'

    print(f"\nüöÄ B·∫Øt ƒë·∫ßu t√≠nh to√°n v√† n·∫°p '{description}' t·ª´ c√°c file CSV local...")

    # 1. Ki·ªÉm tra c√°c file CSV
    if not os.path.exists(movies_csv_path) or not os.path.exists(ratings_csv_path):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file movies.csv ho·∫∑c ratings.csv t·∫°i th∆∞ m·ª•c '{config.DATA_DIR_LOCAL}'.")
        return

    # 2. Ki·ªÉm tra b·∫£ng HBase
    if table_name.encode() not in connection.tables():
         print(f"‚ùå L·ªói: B·∫£ng '{table_name}' ch∆∞a t·ªìn t·∫°i. Vui l√≤ng t·∫°o b·∫£ng trong HBase Shell tr∆∞·ªõc: create '{table_name}', '{cf.decode()}'")
         return

    try:
        print("   -> ƒêang ƒë·ªçc CSV v√† t√≠nh to√°n t·ªïng quan b·∫±ng Pandas...")

        # a. ƒê·∫øm s·ªë phim (Ch·ªâ c·∫ßn ƒë·ªçc 1 c·ªôt b·∫•t k·ª≥ ƒë·ªÉ ƒë·∫øm d√≤ng)
        movies_df = pd.read_csv(movies_csv_path, usecols=['movieId'])
        movie_count = len(movies_df)
        print(f"      - T·ªïng s·ªë phim: {movie_count:,}")

        # b. ƒê·∫øm s·ªë rating v√† user (Ch·ªâ ƒë·ªçc c√°c c·ªôt c·∫ßn thi·∫øt)
        ratings_df = pd.read_csv(ratings_csv_path, usecols=['userId', 'rating'])
        rating_count = len(ratings_df)
        user_count = ratings_df['userId'].nunique() # ƒê·∫øm s·ªë user duy nh·∫•t
        print(f"      - T·ªïng l∆∞·ª£t ƒë√°nh gi√°: {rating_count:,}")
        print(f"      - T·ªïng ng∆∞·ªùi d√πng (unique): {user_count:,}")

        # 3. Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ Put v√†o HBase
        table = connection.table(table_name)

        # D·ªØ li·ªáu ph·∫£i ƒë∆∞·ª£c encode sang bytes
        data_to_put = {
            f'{cf.decode()}:user_count'.encode(): str(user_count).encode(),
            f'{cf.decode()}:movie_count'.encode(): str(movie_count).encode(),
            f'{cf.decode()}:rating_count'.encode(): str(rating_count).encode(),
        }

        # 4. Th·ª±c hi·ªán 1 l·ªánh Put duy nh·∫•t
        print(f"   -> ƒêang ghi d·ªØ li·ªáu v√†o b·∫£ng '{table_name}' v·ªõi RowKey '{row_key.decode()}'...")
        table.put(row_key, data_to_put)

        print(f"‚úÖ HO√ÄN T·∫§T '{description}'!")

    except Exception as e:
        print(f"‚ùå L·ªói khi t√≠nh to√°n t·ªïng quan ho·∫∑c ghi HBase: {e}")


def load_genre_stats(connection):
    """N·∫°p k·∫øt qu·∫£ th·ªëng k√™ th·ªÉ lo·∫°i t·ª´ HDFS v√†o b·∫£ng genre_stats"""
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu n·∫°p 'Th·ªëng K√™ Th·ªÉ Lo·∫°i' t·ª´: {config.HDFS_OUTPUT_GENRES}")

    table = connection.table(config.HBASE_TABLE_GENRE_STATS)
    hdfs_cmd = f"hdfs dfs -cat {config.HDFS_OUTPUT_GENRES}/part-*"

    try:
        process = subprocess.Popen(hdfs_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

        batch = table.batch(batch_size=100)
        count = 0

        for line in process.stdout:
            try:
                line_str = line.decode('utf-8').strip()
                if not line_str: continue

                parts = line_str.split('\t')
                if len(parts) < 2:
                    parts = line_str.split(',')

                if len(parts) >= 2:
                    genre = parts[0].strip()
                    count_val = parts[1].strip()

                    batch.put(genre.encode(), {
                        b'info:count': count_val.encode()
                    })
                    count += 1

            except Exception as e:
                print(f"L·ªói x·ª≠ l√Ω d√≤ng: {e}")
                continue

        batch.send()
        print(f"‚úÖ HO√ÄN T·∫§T 'Th·ªëng K√™ Th·ªÉ Lo·∫°i'! T·ªïng c·ªông: {count} th·ªÉ lo·∫°i.")

    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc HDFS: {e}")

def main():
    # T·∫°o k·∫øt n·ªëi m·ªôt l·∫ßn v√† d√πng chung
    conn = get_hbase_connection()
    if not conn: return

    try:
        # --- C√ÅC JOB C≈® (ƒê·ªçc t·ª´ HDFS MR Output) ---
        # 1. N·∫†P AVG RATING
        load_from_hdfs(
            conn, 
            config.HDFS_OUTPUT_AVG, 
            "stats", 
            "avg_rating", 
            "ƒêi·ªÉm C·ªông ƒê·ªìng (Avg Rating)"
        )

        # 2. N·∫†P RATING COUNT
        load_from_hdfs(
            conn, 
            config.HDFS_OUTPUT_RATINGS, 
            "stats", 
            "rating_count", 
            "S·ªë L∆∞·ª£t ƒê√°nh Gi√° (Rating Count)"
        )

        # --- C√ÅC JOB M·ªöI (ƒê·ªçc t·ª´ CSV Local ƒë·ªÉ th·ªëng k√™ nhanh) ---
        # 3. N·∫†P RATING DISTRIBUTION
        load_rating_distribution_from_csv(conn)

        # 4. N·∫†P SYSTEM OVERVIEW (M·ªöI TH√äM)
        load_system_overview_from_csv(conn)

        # 3. N·∫†P GENRE STATS (Ph√¢n b·ªë th·ªÉ lo·∫°i)
        # MapReduce Job: Count Genres
        load_genre_stats(conn)

    finally:
        # Lu√¥n ƒë√≥ng k·∫øt n·ªëi cu·ªëi c√πng
        if conn:
            conn.close()
            print("\nüîå ƒê√£ ƒë√≥ng k·∫øt n·ªëi HBase.")

if __name__ == "__main__":
    main()